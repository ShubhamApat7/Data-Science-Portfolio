<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>BookMyShow Scraper | Shubham Apat</title>
<meta name=keywords content="Web Scraping,Selenium,BeautifulSoup,Python,Multithreading"><meta name=description content="How I scraped BookMyShow's highly secured platform and achieved 6x performance improvement using a clever Windows GUI trick"><meta name=author content><link rel=canonical href=https://shubhamapat.github.io/projects/bookmyshow-scraper/><link crossorigin=anonymous href=/assets/css/stylesheet.5e650fcd2a1a270f991667f42593d548f47b391d604c83a6bdf0570f9f968095.css integrity="sha256-XmUPzSoaJw+ZFmf0JZPVSPR7OR1gTIOmvfBXD5+WgJU=" rel="preload stylesheet" as=style><link rel=icon href=https://shubhamapat.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shubhamapat.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shubhamapat.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://shubhamapat.github.io/apple-touch-icon.png><link rel=mask-icon href=https://shubhamapat.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://shubhamapat.github.io/projects/bookmyshow-scraper/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://shubhamapat.github.io/projects/bookmyshow-scraper/"><meta property="og:site_name" content="Shubham Apat"><meta property="og:title" content="BookMyShow Scraper"><meta property="og:description" content="How I scraped BookMyShow's highly secured platform and achieved 6x performance improvement using a clever Windows GUI trick"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-04-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-05T00:00:00+00:00"><meta property="article:tag" content="Web Scraping"><meta property="article:tag" content="Selenium"><meta property="article:tag" content="BeautifulSoup"><meta property="article:tag" content="Python"><meta property="article:tag" content="Multithreading"><meta name=twitter:card content="summary"><meta name=twitter:title content="BookMyShow Scraper"><meta name=twitter:description content="How I scraped BookMyShow's highly secured platform and achieved 6x performance improvement using a clever Windows GUI trick"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://shubhamapat.github.io/projects/"},{"@type":"ListItem","position":2,"name":"BookMyShow Scraper","item":"https://shubhamapat.github.io/projects/bookmyshow-scraper/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"BookMyShow Scraper","name":"BookMyShow Scraper","description":"How I scraped BookMyShow's highly secured platform and achieved 6x performance improvement using a clever Windows GUI trick","keywords":["Web Scraping","Selenium","BeautifulSoup","Python","Multithreading"],"articleBody":"I had never made a scraper and the first scraper i was assigned to build was BookMyShow. I was asked to scrape the every event listed on all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nBookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges So as a newbie I started research on how to build a scraper from scratch, went on reddit, stack.com, medium and documentations of beautifulsoup, playwright and selenium. So I built a simple two step scraper, first step which can scrape just events urls from their cards using the selenium chromedriver and find the div class of card using beautifulsoup.\nThe second step takes this scraped urls of cards, opens them one by one and scrape information like date, time, venue, language, age-limit, price and even about the event into a csv file. All of this using BeautifulSoup4.\nOnly problem with the first attempt was that the first step was not able to scroll much further because BMS uses lazy loading. Then again I started research about how to scrape sites that uses lazy loading for scrolling down the page to trigger the loading of dynamic content. Addition to lazy-loading, bookmyshow has an advance bot detection, if you keep scrolling to the bottom of the page for all the event cards at same pace it assumes you’re bot and stops loading events!!\nTo tackle this I came up with my own human-like scroll method for bot after a lot of trial and errors. This human scroll makes the bot scroll the page to bottom till footer and as pages load more event cards it again scroll up to get the urls from those cards and then again repeat the whole loop till no more new cards are loaded, and the pace of scrolling is random everytime so that avoids bot detection.\ndef human_scroll(driver): actions = ActionChains(driver) for _ in range(random.randint(5, 8)): actions.send_keys(Keys.ARROW_DOWN).perform() time.sleep(random.uniform(0.08, 0.2)) The scraper was ready was ready but there are still optimizations needed, cause I needed to 1898 cities in 4 different categories(sports, events, plays, and activities). To scrape a city’s events, sports, activities and plays page one by one was taking 120 seconds overall, now 120x1898 = 64 hours or 2.63 days. Ofcourse it needs optimization!\nIf you have read my other blogs, the zeroth step I take before optimise a pipeline is by measuring time taken by each function using time.time(). I got to know each categories page was taking around 35s to load and get scraped. To reduce, I decreased the sleep() time for loading and scrolling of scraper, which reduced 30s to around 25s but its still 100s overall. I couldn’t decrease the sleep() time for scrolling beyond that because then it won’t wait for lazy loading to load more events anymore and exit first.\nAnother optimization I applied was multithreading where I can scrape all 4 categories(events, sports, plays and activities) parallel that reduced the time 100s/4 t= 25s. With that I applied some methods to avoid collecting duplicate data where it does not save the same card url if thats scraped before which reduce unnecessary reads/writes, skip the city if there are no new cards and made scraper resumable. Only problem was we had to keep every window running parallel in foreground if the window is running in background BMS somehow stops loading events IKR!!\nso i used windows 4-screens layout for scraping in parallel\ndriver_events.set_window_position(0, 0) # top-left driver_sports.set_window_position(960, 0) # top-right driver_activities.set_window_position(0, 540) # bottom-left driver_plays.set_window_position(960, 540) # bottom-right The final time was 18s per city which means 18*1898= 9.5 hours so finaly we optimized data scraping from 64 hours to 9.5 hours!\nHere’s a snap of my bookmyshow scraper:\nYour browser does not support the video tag. Key Takeaway Sometimes the best optimization isn’t faster code—it’s a different approach altogether.\nThis scraper collected comprehensive event data from India’s largest entertainment platform.\n","wordCount":"659","inLanguage":"en","datePublished":"2024-04-05T00:00:00Z","dateModified":"2024-04-05T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://shubhamapat.github.io/projects/bookmyshow-scraper/"},"publisher":{"@type":"Organization","name":"Shubham Apat","logo":{"@type":"ImageObject","url":"https://shubhamapat.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://shubhamapat.github.io/ accesskey=h title="Shubham Apat (Alt + H)">Shubham Apat</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shubhamapat.github.io/post/ title=Posts><span>Posts</span></a></li><li><a href=https://shubhamapat.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://shubhamapat.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://github.com/shubhamapat7 title=GitHub><span>GitHub</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">BookMyShow Scraper</h1><div class=post-description>How I scraped BookMyShow's highly secured platform and achieved 6x performance improvement using a clever Windows GUI trick</div><div class=post-meta><span title='2024-04-05 00:00:00 +0000 UTC'>April 5, 2024</span>&nbsp;·&nbsp;<span>4 min</span></div></header><div class=post-content><p>I had never made a scraper and the first scraper i was assigned to build was BookMyShow. I was asked to scrape the every event listed on <strong>all 1,898 cities</strong> in India across 4 categories (plays, sports, events, activities).</p><p>BookMyShow is <strong>notoriously difficult</strong> to scrape. They have:</p><ul><li>Advanced bot detection</li><li>Dynamic JavaScript content</li><li>Rate limiting</li><li>CAPTCHA challenges</li></ul><p>So as a newbie I started research on how to build a scraper from scratch, went on reddit, stack.com, medium and documentations of beautifulsoup, playwright and selenium. So I built a simple two step scraper, first step which can scrape just events urls from their cards using the selenium chromedriver and find the div class of card using beautifulsoup.</p><img src=/images/bms.png><p>The second step takes this scraped urls of cards, opens them one by one and scrape information like date, time, venue, language, age-limit, price and even about the event into a csv file. All of this using <strong>BeautifulSoup4</strong>.</p><img src=/images/bms-url.png><p>Only problem with the first attempt was that the first step was not able to scroll much further because BMS uses <a href=https://www.cloudflare.com/learning/performance/what-is-lazy-loading/>lazy loading</a>. Then again I started research about how to scrape sites that uses lazy loading for scrolling down the page to trigger the loading of dynamic content. Addition to lazy-loading, bookmyshow has an advance bot detection, if you keep scrolling to the bottom of the page for all the event cards at same pace it assumes you&rsquo;re bot and stops loading events!!</p><p>To tackle this I came up with my own human-like scroll method for bot after a lot of trial and errors. This human scroll makes the bot scroll the page to bottom till footer and as pages load more event cards it again scroll up to get the urls from those cards and then again repeat the whole loop till no more new cards are loaded, and the pace of scrolling is random everytime so that avoids bot detection.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>human_scroll</span>(driver):
</span></span><span style=display:flex><span>    actions <span style=color:#f92672>=</span> ActionChains(driver)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>8</span>)):
</span></span><span style=display:flex><span>        actions<span style=color:#f92672>.</span>send_keys(Keys<span style=color:#f92672>.</span>ARROW_DOWN)<span style=color:#f92672>.</span>perform()
</span></span><span style=display:flex><span>        time<span style=color:#f92672>.</span>sleep(random<span style=color:#f92672>.</span>uniform(<span style=color:#ae81ff>0.08</span>, <span style=color:#ae81ff>0.2</span>))
</span></span></code></pre></div><p>The scraper was ready was ready but there are still optimizations needed, cause I needed to 1898 cities in 4 different categories(sports, events, plays, and activities). To scrape a city&rsquo;s events, sports, activities and plays page one by one was taking 120 seconds overall, now 120x1898 = 64 hours or 2.63 days. Ofcourse it needs optimization!</p><p>If you have read my other blogs, the zeroth step I take before optimise a pipeline is by measuring time taken by each function using time.time(). I got to know each categories page was taking around 35s to load and get scraped. To reduce, I decreased the sleep() time for loading and scrolling of scraper, which reduced 30s to around 25s but its still 100s overall. I couldn&rsquo;t decrease the sleep() time for scrolling beyond that because then it won&rsquo;t wait for lazy loading to load more events anymore and exit first.</p><p>Another optimization I applied was multithreading where I can scrape all 4 categories(events, sports, plays and activities) parallel that reduced the time 100s/4 t= 25s. With that I applied some methods to avoid collecting duplicate data where it does not save the same card url if thats scraped before which reduce unnecessary reads/writes, skip the city if there are no new cards and made scraper resumable. Only problem was we had to keep every window running parallel in foreground if the window is running in background BMS somehow stops loading events IKR!!</p><p>so i used windows 4-screens layout for scraping in parallel</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    driver_events<span style=color:#f92672>.</span>set_window_position(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>)       <span style=color:#75715e># top-left</span>
</span></span><span style=display:flex><span>    driver_sports<span style=color:#f92672>.</span>set_window_position(<span style=color:#ae81ff>960</span>, <span style=color:#ae81ff>0</span>)     <span style=color:#75715e># top-right</span>
</span></span><span style=display:flex><span>    driver_activities<span style=color:#f92672>.</span>set_window_position(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>540</span>) <span style=color:#75715e># bottom-left</span>
</span></span><span style=display:flex><span>    driver_plays<span style=color:#f92672>.</span>set_window_position(<span style=color:#ae81ff>960</span>, <span style=color:#ae81ff>540</span>)    <span style=color:#75715e># bottom-right</span>
</span></span></code></pre></div><p>The final time was 18s per city which means 18*1898= 9.5 hours so finaly we optimized data scraping from 64 hours to 9.5 hours!</p><p>Here&rsquo;s a snap of my bookmyshow scraper:</p><video controls width=100% style="margin:20px 0">
<source src=/bms.mp4 type=video/mp4>Your browser does not support the video tag.</video><h2 id=key-takeaway>Key Takeaway<a hidden class=anchor aria-hidden=true href=#key-takeaway>#</a></h2><p>Sometimes the best optimization isn&rsquo;t faster code—it&rsquo;s a <strong>different approach altogether</strong>.</p><hr><p><em>This scraper collected comprehensive event data from India&rsquo;s largest entertainment platform.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://shubhamapat.github.io/tags/web-scraping/>Web Scraping</a></li><li><a href=https://shubhamapat.github.io/tags/selenium/>Selenium</a></li><li><a href=https://shubhamapat.github.io/tags/beautifulsoup/>BeautifulSoup</a></li><li><a href=https://shubhamapat.github.io/tags/python/>Python</a></li><li><a href=https://shubhamapat.github.io/tags/multithreading/>Multithreading</a></li></ul><nav class=paginav><a class=prev href=https://shubhamapat.github.io/projects/individual-identification-of-mugger-crocodile/><span class=title>« Prev</span><br><span>Individual Identification of Mugger Crocodiles using YOLOv8</span>
</a><a class=next href=https://shubhamapat.github.io/projects/remote-sensing-mangrove-classification/><span class=title>Next »</span><br><span>Remote Sensing-Based Mangrove Classification</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://shubhamapat.github.io/>Shubham Apat</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>