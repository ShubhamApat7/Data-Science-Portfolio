<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Individual Identification of Mugger Crocodiles using YOLOv8 | Shubham Apat</title>
<meta name=keywords content="Deep Learning,Computer Vision,Research"><meta name=description content="Abstract

Individual identification contributes significantly towards investigating behavioral mechanisms of animals and understanding underlying ecological principles. Most studies employ invasive procedures for individually identifying organisms. In recent times, computer-vision techniques have served as an alternative to invasive methods. However, these studies primarily rely on user input data collected from captivity or from individuals under partially restrained conditions. Challenges in collecting data from free-ranging individuals are higher when compared to captive populations. However, the former is a far more important priority for real-world applications."><meta name=author content><link rel=canonical href=https://shubhamapat.github.io/projects/individual-identification-of-mugger-crocodile/><link crossorigin=anonymous href=/assets/css/stylesheet.5e650fcd2a1a270f991667f42593d548f47b391d604c83a6bdf0570f9f968095.css integrity="sha256-XmUPzSoaJw+ZFmf0JZPVSPR7OR1gTIOmvfBXD5+WgJU=" rel="preload stylesheet" as=style><link rel=icon href=https://shubhamapat.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shubhamapat.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shubhamapat.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://shubhamapat.github.io/apple-touch-icon.png><link rel=mask-icon href=https://shubhamapat.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://shubhamapat.github.io/projects/individual-identification-of-mugger-crocodile/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://shubhamapat.github.io/projects/individual-identification-of-mugger-crocodile/"><meta property="og:site_name" content="Shubham Apat"><meta property="og:title" content="Individual Identification of Mugger Crocodiles using YOLOv8"><meta property="og:description" content="Abstract
Individual identification contributes significantly towards investigating behavioral mechanisms of animals and understanding underlying ecological principles. Most studies employ invasive procedures for individually identifying organisms. In recent times, computer-vision techniques have served as an alternative to invasive methods. However, these studies primarily rely on user input data collected from captivity or from individuals under partially restrained conditions. Challenges in collecting data from free-ranging individuals are higher when compared to captive populations. However, the former is a far more important priority for real-world applications."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-04-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-20T00:00:00+00:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Research"><meta property="og:image" content="https://shubhamapat.github.io/images/mugger_cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shubhamapat.github.io/images/mugger_cover.jpg"><meta name=twitter:title content="Individual Identification of Mugger Crocodiles using YOLOv8"><meta name=twitter:description content="Abstract

Individual identification contributes significantly towards investigating behavioral mechanisms of animals and understanding underlying ecological principles. Most studies employ invasive procedures for individually identifying organisms. In recent times, computer-vision techniques have served as an alternative to invasive methods. However, these studies primarily rely on user input data collected from captivity or from individuals under partially restrained conditions. Challenges in collecting data from free-ranging individuals are higher when compared to captive populations. However, the former is a far more important priority for real-world applications."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://shubhamapat.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Individual Identification of Mugger Crocodiles using YOLOv8","item":"https://shubhamapat.github.io/projects/individual-identification-of-mugger-crocodile/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Individual Identification of Mugger Crocodiles using YOLOv8","name":"Individual Identification of Mugger Crocodiles using YOLOv8","description":"Abstract\nIndividual identification contributes significantly towards investigating behavioral mechanisms of animals and understanding underlying ecological principles. Most studies employ invasive procedures for individually identifying organisms. In recent times, computer-vision techniques have served as an alternative to invasive methods. However, these studies primarily rely on user input data collected from captivity or from individuals under partially restrained conditions. Challenges in collecting data from free-ranging individuals are higher when compared to captive populations. However, the former is a far more important priority for real-world applications.\n","keywords":["Deep Learning","Computer Vision","Research"],"articleBody":"Abstract\nIndividual identification contributes significantly towards investigating behavioral mechanisms of animals and understanding underlying ecological principles. Most studies employ invasive procedures for individually identifying organisms. In recent times, computer-vision techniques have served as an alternative to invasive methods. However, these studies primarily rely on user input data collected from captivity or from individuals under partially restrained conditions. Challenges in collecting data from free-ranging individuals are higher when compared to captive populations. However, the former is a far more important priority for real-world applications.\nIndividual Identification of mugger crocodile, is where my computer vision journey started. I was part of a deep learning research project under Prof. Mehul Raval, where we were provided a dataset of free-ranging muggger crocodiles collected using Unmanned Aerial Vehicle (UAV). The dataset contained total 160,000 images focusing on mugger’s dorsal body. The data was collected from 160 individuals across 19 different locations along the western part of India. This was an extension to an already done research.\nUsing a CNN model, we aim to individually identify free ranging mugger crocodiles, Crocodylus palustris based on their dorsal scute patterns. Scutes are hard calcium bony plates called osteoderms (Fig. 1) that form a crocodile’s dorsal body. It serves as a unique identifier based on its placement on the dorsal body. With this background, we hypothesize that the developed CNN-based IID for mugger crocodiles will provide a robust understanding of behavioral processes and physiological patterns at an individual level, which in turn will contribute towards species conservation.\nThe dataset had 160,000 images and their annotations (bounding boxes of scute patterns), we decided to use YOLOv8 as it is the state of the art object detection model, known for its speed and accuracy. The size of dataset was ~450GB, for data preprocessing and model training we were assigned access to ParamShavak supercoputer which use NVIDIA A6000 GPU!\nOnly problem paramshavak did not have GUI, so we coded whole summer in pUTTY cli, doing image processing tasks on a machine where images couldn’t open, very ironic right!\nData Preprocessing\nStandard data preprocessing steps before any object detection model training are image processing which includes, image resizing, normalization and data augmentation. Also annotation processing is very important step, Adjust bounding box coordinates to match the image transformations applied during preprocessing (e.g., resizing, cropping, flipping). Format Conversion, Convert annotations to the format expected by the chosen object detection framework (e.g., COCO, PASCAL VOC, YOLO format).\nAs we were using YOLOv8, we resized every image to 640x640, applied normalization and converted annotations into YOLO txt format. Another step taken was out of 1000 images per class, we only used 250 images per class in training by taking every 4th frame, divided that train data into 90:10 ratio for train and validation. Other 750 images per class were taken as unseen testing data.\nModel Training\nSince we already had selected the model, next step was model configuration. Configured hyperparameters using Adam optimizer, batch normalization and 100 training epochs.\nFor yolov8 training you need to prepare a YAML mapping file where you need to give path to train data, val data and mapping of classes.\nyolo train model=yolov8m.pt data=mapping.yaml epochs=100 imgsz=640 This was first training version which acheived 97.5% mAP50, after that did hyperparameter tuning with train-settings and the images-per-class which varied from 50,100,150,200,500 and even taking all 1000. But the final best accuracy acheived was with optimizer Adam, aumentations settings like perspective, flipud, fliplr, scale and 250 images per class whcih acheived 99.5% mAP50.\nFor testing, we used new data collected six months after the training dataset. Because of the time gap, the weather conditions had changed significantly, creating a noticeable difference between the training and test images.\nWe used two parameters, True Positive Rate (TPR) and True Negative Rate (TNR), to validate the efficiency of the trained models. Using YOLO-v5l, TPR (re-identification of trained muggers) and TNR (differentiating untrained muggers as ‘unknown’) values at the 0.84 decision threshold were 88.8% and 89.6%, respectively. The trained model showed 100% TNR for the non-mugger species, the Gharial, Gavialis gangeticus, and the Saltwater crocodile, Crocodylus porosus.\nMetrics\nThis confusion matrix displays the predicted values on the y-axis and the true values on the x-axis. The diagonal line represents perfect predictions, where the predicted value matches the true value.\nprecision(B): This metric shows some fluctuations but generally increases over time, indicating improving precision performance. recall(B): Similar to precision, the recall metric also improves as training progresses. mAP50(B): This metric, likely mean Average Precision with an IoU threshold of 0.5, starts low but steadily increases, suggesting better object detection/localization performance. mAP50-95(B): This metric, probably mean Average Precision averaged over multiple IoU thresholds (0.5 to 0.95), also improves consistently during training.\nThe performance of the CNN model was reliable and accurate while using only 250 images per individual for training purposes. Showing that a bounding box approach (YOLO-v5l model) with background elimination is a promising method to individually identify free-ranging mugger crocodiles. Our manuscript demonstrates that UAV imagery appears to be a promising tool for non-invasive collection of data from free-ranging populations. It can be used to train open-source algorithms for individual identification. Further, the identification method is entirely based upon dorsal scute patterns, which can be applied to different crocodilian species, as well.\n","wordCount":"874","inLanguage":"en","image":"https://shubhamapat.github.io/images/mugger_cover.jpg","datePublished":"2024-04-20T00:00:00Z","dateModified":"2024-04-20T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://shubhamapat.github.io/projects/individual-identification-of-mugger-crocodile/"},"publisher":{"@type":"Organization","name":"Shubham Apat","logo":{"@type":"ImageObject","url":"https://shubhamapat.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://shubhamapat.github.io/ accesskey=h title="Shubham Apat (Alt + H)">Shubham Apat</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shubhamapat.github.io/post/ title=Posts><span>Posts</span></a></li><li><a href=https://shubhamapat.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://shubhamapat.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://github.com/shubhamapat7 title=GitHub><span>GitHub</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Individual Identification of Mugger Crocodiles using YOLOv8</h1><div class=post-meta><span title='2024-04-20 00:00:00 +0000 UTC'>April 20, 2024</span>&nbsp;·&nbsp;<span>5 min</span></div></header><figure class=entry-cover><img loading=eager src=https://shubhamapat.github.io/images/mugger_cover.jpg alt></figure><div class=post-content><p><strong>Abstract</strong></p><blockquote><p>Individual identification contributes significantly towards investigating behavioral mechanisms of animals and understanding underlying ecological principles. Most studies employ invasive procedures for individually identifying organisms. In recent times, computer-vision techniques have served as an alternative to invasive methods. However, these studies primarily rely on user input data collected from captivity or from individuals under partially restrained conditions. Challenges in collecting data from free-ranging individuals are higher when compared to captive populations. However, the former is a far more important priority for real-world applications.</p></blockquote><p>Individual Identification of mugger crocodile, is where my computer vision journey started. I was part of a deep learning research project under <strong>Prof. Mehul Raval</strong>, where we were provided a dataset of free-ranging muggger crocodiles collected using Unmanned Aerial Vehicle (UAV). The dataset contained total 160,000 images focusing on mugger&rsquo;s dorsal body. The data was collected from 160 individuals across 19 different locations along the western part of India.
This was an extension to an already done <a href=https://india.mongabay.com/2022/11/identifying-individual-mugger-crocodiles-using-drone-technology-to-minimise-conflicts/>research</a>.</p><img src=/images/mugger.png width=300><p>Using a CNN model, we aim to individually identify free ranging mugger crocodiles, Crocodylus palustris based on their dorsal scute patterns. Scutes are hard calcium bony plates called osteoderms (Fig. 1) that form a crocodile&rsquo;s dorsal body. It serves as a unique identifier based on its placement on the dorsal body. With this background, we hypothesize that the developed CNN-based IID for mugger crocodiles will provide a robust understanding of behavioral processes and physiological patterns at an individual level, which in turn will contribute towards species conservation.</p><p>The dataset had 160,000 images and their annotations (bounding boxes of scute patterns), we decided to use YOLOv8 as it is the state of the art object detection model, known for its speed and accuracy. The size of dataset was ~450GB, for data preprocessing and model training we were assigned access to ParamShavak supercoputer which use NVIDIA A6000 GPU!</p><blockquote><p><strong>Only problem paramshavak did not have GUI, so we coded whole summer in pUTTY cli, doing image processing tasks on a machine where images couldn&rsquo;t open, very ironic right!</strong></p></blockquote><p><strong>Data Preprocessing</strong></p><p>Standard data preprocessing steps before any object detection model training are image processing which includes, image resizing, normalization and data augmentation. Also annotation processing is very important step, Adjust bounding box coordinates to match the image transformations applied during preprocessing (e.g., resizing, cropping, flipping). Format Conversion, Convert annotations to the format expected by the chosen object detection framework (e.g., COCO, PASCAL VOC, YOLO format).</p><p>As we were using YOLOv8, we resized every image to 640x640, applied normalization and converted annotations into YOLO txt format. Another step taken was out of 1000 images per class, we only used 250 images per class in training by taking every 4th frame, divided that train data into 90:10 ratio for train and validation. Other 750 images per class were taken as unseen testing data.</p><p><strong>Model Training</strong></p><p>Since we already had selected the model, next step was model configuration. Configured hyperparameters using Adam optimizer, batch normalization and 100 training epochs.</p><p>For yolov8 training you need to prepare a YAML mapping file where you need to give path to train data, val data and mapping of classes.</p><pre tabindex=0><code class=language-cli data-lang=cli>yolo train model=yolov8m.pt data=mapping.yaml epochs=100 imgsz=640
</code></pre><p>This was first training version which acheived 97.5% mAP50, after that did hyperparameter tuning with <a href=%22https://docs.ultralytics.com/modes/train/#train-settings%22>train-settings</a> and the images-per-class which varied from 50,100,150,200,500 and even taking all 1000. But the final best accuracy acheived was with optimizer Adam, aumentations settings like perspective, flipud, fliplr, scale and 250 images per class whcih acheived <strong>99.5% mAP50</strong>.</p><p>For testing, we used new data collected six months after the training dataset. Because of the time gap, the weather conditions had changed significantly, creating a noticeable difference between the training and test images.</p><p>We used two parameters, True Positive Rate (TPR) and True Negative Rate (TNR), to validate the efficiency of the trained models. Using YOLO-v5l, TPR (re-identification of trained muggers) and TNR (differentiating untrained muggers as ‘unknown’) values at the 0.84 decision threshold were 88.8% and 89.6%, respectively. The trained model showed 100% TNR for the non-mugger species, the Gharial, Gavialis gangeticus, and the Saltwater crocodile, Crocodylus porosus.</p><img src=/images/yolov8_output1.png width=600>
<img src=/images/yolov8_output2.png width=600><p><strong>Metrics</strong></p><img src=/images/confusion_matrix.png width=600><p>This confusion matrix displays the predicted values on the y-axis and the true values on the x-axis. The diagonal line represents perfect predictions, where the predicted value matches the true value.</p><img src=/images/other_metrics.png><p>precision(B): This metric shows some fluctuations but generally increases over time, indicating improving precision performance. recall(B): Similar to precision, the recall metric also improves as training progresses. mAP50(B): This metric, likely mean Average Precision with an IoU threshold of 0.5, starts low but steadily increases, suggesting better object detection/localization performance. mAP50-95(B): This metric, probably mean Average Precision averaged over multiple IoU thresholds (0.5 to 0.95), also improves consistently during training.</p><p>The performance of the CNN model was reliable and accurate while using only 250 images per individual for training purposes. Showing that a bounding box approach (YOLO-v5l model) with background elimination is a promising method to individually identify free-ranging mugger crocodiles. Our manuscript demonstrates that UAV imagery appears to be a promising tool for non-invasive collection of data from free-ranging populations. It can be used to train open-source algorithms for individual identification. Further, the identification method is entirely based upon dorsal scute patterns, which can be applied to different crocodilian species, as well.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://shubhamapat.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://shubhamapat.github.io/tags/computer-vision/>Computer Vision</a></li><li><a href=https://shubhamapat.github.io/tags/research/>Research</a></li></ul><nav class=paginav><a class=prev href=https://shubhamapat.github.io/projects/college-hotspots-dashboard/><span class=title>« Prev</span><br><span>College Hotspots Dashboard</span>
</a><a class=next href=https://shubhamapat.github.io/projects/bookmyshow-scraper/><span class=title>Next »</span><br><span>BookMyShow Scraper</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://shubhamapat.github.io/>Shubham Apat</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>