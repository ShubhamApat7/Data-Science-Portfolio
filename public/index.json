[{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms.\nChallenge Mangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality Solution Data Collection Strategy Geo point-based sampling: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions Used Global Mangrove Watch data for reference Separated into mangrove/non-mangrove classes Feature Engineering Spectral bands: B2, B3, B4, B8, B11 from Sentinel-2A Indices: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index) Multi-spectral analysis for robust classification Model Performance Algorithm: Random Forest Classifier Accuracy: 99.03% on test data Generalization: Strong performance on unseen regions Deployment Google Earth Engine tile exports for classified imagery Custom Streamlit web application for real-time visualization Interactive classification by user clicks on map Technologies Used Google Earth Engine (GEE) Sentinel-2A Multispectral Imagery Random Forest Streamlit Python Geospatial Analysis ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms.\u003c/p\u003e\n\u003ch2 id=\"challenge\"\u003eChallenge\u003c/h2\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solution\"\u003eSolution\u003c/h2\u003e\n\u003ch3 id=\"data-collection-strategy\"\u003eData Collection Strategy\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeo point-based sampling\u003c/strong\u003e: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions\u003c/li\u003e\n\u003cli\u003eUsed Global Mangrove Watch data for reference\u003c/li\u003e\n\u003cli\u003eSeparated into mangrove/non-mangrove classes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"feature-engineering\"\u003eFeature Engineering\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpectral bands\u003c/strong\u003e: B2, B3, B4, B8, B11 from Sentinel-2A\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndices\u003c/strong\u003e: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index)\u003c/li\u003e\n\u003cli\u003eMulti-spectral analysis for robust classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-performance\"\u003eModel Performance\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAlgorithm\u003c/strong\u003e: Random Forest Classifier\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 99.03% on test data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization\u003c/strong\u003e: Strong performance on unseen regions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"deployment\"\u003eDeployment\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoogle Earth Engine\u003c/strong\u003e tile exports for classified imagery\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom Streamlit web application\u003c/strong\u003e for real-time visualization\u003c/li\u003e\n\u003cli\u003eInteractive classification by user clicks on map\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle Earth Engine (GEE)\u003c/li\u003e\n\u003cli\u003eSentinel-2A Multispectral Imagery\u003c/li\u003e\n\u003cli\u003eRandom Forest\u003c/li\u003e\n\u003cli\u003eStreamlit\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eGeospatial Analysis\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality Solution Data Collection Strategy Geo point-based sampling: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions Used Global Mangrove Watch data for reference Separated into mangrove/non-mangrove classes Feature Engineering Spectral bands: B2, B3, B4, B8, B11 from Sentinel-2A Indices: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index) Multi-spectral analysis for robust classification Model Performance Algorithm: Random Forest Classifier Accuracy: 99.03% on test data Generalization: Strong performance on unseen regions Deployment Google Earth Engine tile exports for classified imagery Custom Streamlit web application for real-time visualization Interactive classification by user clicks on map Technologies Used Google Earth Engine (GEE) Sentinel-2A Multispectral Imagery Random Forest Streamlit Python Geospatial Analysis ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cimg src=/images/mangrove_plantation.png\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality Solution Data Collection Strategy Geo point-based sampling: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions Used Global Mangrove Watch data for reference Separated into mangrove/non-mangrove classes Feature Engineering Spectral bands: B2, B3, B4, B8, B11 from Sentinel-2A Indices: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index) Multi-spectral analysis for robust classification Model Performance Algorithm: Random Forest Classifier Accuracy: 99.03% on test data Generalization: Strong performance on unseen regions Deployment Google Earth Engine tile exports for classified imagery Custom Streamlit web application for real-time visualization Interactive classification by user clicks on map Technologies Used Google Earth Engine (GEE) Sentinel-2A Multispectral Imagery Random Forest Streamlit Python Geospatial Analysis ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cimg src=\"/images/mangrove_plantation.png\"\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solution\"\u003eSolution\u003c/h2\u003e\n\u003ch3 id=\"data-collection-strategy\"\u003eData Collection Strategy\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeo point-based sampling\u003c/strong\u003e: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions\u003c/li\u003e\n\u003cli\u003eUsed Global Mangrove Watch data for reference\u003c/li\u003e\n\u003cli\u003eSeparated into mangrove/non-mangrove classes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"feature-engineering\"\u003eFeature Engineering\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpectral bands\u003c/strong\u003e: B2, B3, B4, B8, B11 from Sentinel-2A\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndices\u003c/strong\u003e: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index)\u003c/li\u003e\n\u003cli\u003eMulti-spectral analysis for robust classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-performance\"\u003eModel Performance\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAlgorithm\u003c/strong\u003e: Random Forest Classifier\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 99.03% on test data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization\u003c/strong\u003e: Strong performance on unseen regions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"deployment\"\u003eDeployment\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoogle Earth Engine\u003c/strong\u003e tile exports for classified imagery\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom Streamlit web application\u003c/strong\u003e for real-time visualization\u003c/li\u003e\n\u003cli\u003eInteractive classification by user clicks on map\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle Earth Engine (GEE)\u003c/li\u003e\n\u003cli\u003eSentinel-2A Multispectral Imagery\u003c/li\u003e\n\u003cli\u003eRandom Forest\u003c/li\u003e\n\u003cli\u003eStreamlit\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eGeospatial Analysis\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality Solution Data Collection Strategy Geo point-based sampling: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions Used Global Mangrove Watch data for reference Separated into mangrove/non-mangrove classes Feature Engineering Spectral bands: B2, B3, B4, B8, B11 from Sentinel-2A Indices: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index) Multi-spectral analysis for robust classification Model Performance Algorithm: Random Forest Classifier Accuracy: 99.03% on test data Generalization: Strong performance on unseen regions Deployment Google Earth Engine tile exports for classified imagery Custom Streamlit web application for real-time visualization Interactive classification by user clicks on map Technologies Used Google Earth Engine (GEE) Sentinel-2A Multispectral Imagery Random Forest Streamlit Python Geospatial Analysis ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solution\"\u003eSolution\u003c/h2\u003e\n\u003ch3 id=\"data-collection-strategy\"\u003eData Collection Strategy\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeo point-based sampling\u003c/strong\u003e: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions\u003c/li\u003e\n\u003cli\u003eUsed Global Mangrove Watch data for reference\u003c/li\u003e\n\u003cli\u003eSeparated into mangrove/non-mangrove classes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"feature-engineering\"\u003eFeature Engineering\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpectral bands\u003c/strong\u003e: B2, B3, B4, B8, B11 from Sentinel-2A\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndices\u003c/strong\u003e: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index)\u003c/li\u003e\n\u003cli\u003eMulti-spectral analysis for robust classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-performance\"\u003eModel Performance\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAlgorithm\u003c/strong\u003e: Random Forest Classifier\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 99.03% on test data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization\u003c/strong\u003e: Strong performance on unseen regions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"deployment\"\u003eDeployment\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoogle Earth Engine\u003c/strong\u003e tile exports for classified imagery\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom Streamlit web application\u003c/strong\u003e for real-time visualization\u003c/li\u003e\n\u003cli\u003eInteractive classification by user clicks on map\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle Earth Engine (GEE)\u003c/li\u003e\n\u003cli\u003eSentinel-2A Multispectral Imagery\u003c/li\u003e\n\u003cli\u003eRandom Forest\u003c/li\u003e\n\u003cli\u003eStreamlit\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eGeospatial Analysis\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality Solution Data Collection Strategy Geo point-based sampling: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions Used Global Mangrove Watch data for reference Separated into mangrove/non-mangrove classes Feature Engineering Spectral bands: B2, B3, B4, B8, B11 from Sentinel-2A Indices: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index) Multi-spectral analysis for robust classification Model Performance Algorithm: Random Forest Classifier Accuracy: 99.03% on test data Generalization: Strong performance on unseen regions Deployment Google Earth Engine tile exports for classified imagery Custom Streamlit web application for real-time visualization Interactive classification by user clicks on map Technologies Used Google Earth Engine (GEE) Sentinel-2A Multispectral Imagery Random Forest Streamlit Python Geospatial Analysis ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solution\"\u003eSolution\u003c/h2\u003e\n\u003ch3 id=\"data-collection-strategy\"\u003eData Collection Strategy\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeo point-based sampling\u003c/strong\u003e: Over 1,100 manually labeled points across India\u0026rsquo;s coastal regions\u003c/li\u003e\n\u003cli\u003eUsed Global Mangrove Watch data for reference\u003c/li\u003e\n\u003cli\u003eSeparated into mangrove/non-mangrove classes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"feature-engineering\"\u003eFeature Engineering\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpectral bands\u003c/strong\u003e: B2, B3, B4, B8, B11 from Sentinel-2A\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndices\u003c/strong\u003e: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index)\u003c/li\u003e\n\u003cli\u003eMulti-spectral analysis for robust classification\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model-performance\"\u003eModel Performance\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAlgorithm\u003c/strong\u003e: Random Forest Classifier\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e: 99.03% on test data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGeneralization\u003c/strong\u003e: Strong performance on unseen regions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"deployment\"\u003eDeployment\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGoogle Earth Engine\u003c/strong\u003e tile exports for classified imagery\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCustom Streamlit web application\u003c/strong\u003e for real-time visualization\u003c/li\u003e\n\u003cli\u003eInteractive classification by user clicks on map\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technologies-used\"\u003eTechnologies Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle Earth Engine (GEE)\u003c/li\u003e\n\u003cli\u003eSentinel-2A Multispectral Imagery\u003c/li\u003e\n\u003cli\u003eRandom Forest\u003c/li\u003e\n\u003cli\u003eStreamlit\u003c/li\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eGeospatial Analysis\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection The classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as: â–  Sundarbans â–  Gulf of Khambhat â–  Gulf of Mannar â–  Pichavaram â–  Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range: â–  Start date: January 1, 2023 â–  End date: January 1, 2024 â–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as: â–  Sundarbans â–  Gulf of Khambhat â–  Gulf of Mannar â–  Pichavaram â–  Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range: â–  Start date: January 1, 2023 â–  End date: January 1, 2024 â–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nâ–  Sundarbans â–  Gulf of Khambhat â–  Gulf of Mannar â–  Pichavaram â–  Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range: â–  Start date: January 1, 2023 â–  End date: January 1, 2024 â–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans -Gulf of Khambhat -Gulf of Mannar -Pichavaram -Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range: â–  Start date: January 1, 2023 â–  End date: January 1, 2024 â–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range: â–  Start date: January 1, 2023 â–  End date: January 1, 2024 â–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range:\nâ–  Start date: January 1, 2023\nâ–  End date: January 1, 2024\nâ–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range:\nâ–  Start date: January 1, 2023\nâ–  End date: January 1, 2024\nâ–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nâ–  Mangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nâ–  Non-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nâ–  Points were stored using custom Google Earth Engine scripts along with spatial metadata.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range:\nâ–  Start date: January 1, 2023\nâ–  End date: January 1, 2024\nâ–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nâ–  Mangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nâ–  Non-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nâ–  Points were stored using custom Google Earth Engine scripts along with spatial metadata. ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery. It was filtered for the temporal range:\nâ–  Start date: January 1, 2023\nâ–  End date: January 1, 2024\nâ–  Cloud cover: Less than 10% Composite images were generated using the median reducer to minimize residual cloud and atmospheric noise.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nâ–  Mangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nâ–  Non-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nâ–  Points were stored using custom Google Earth Engine scripts along with spatial metadata. ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nâ–  Mangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nâ–  Non-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nâ–  Points were stored using custom Google Earth Engine scripts along with spatial metadata. ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nPoints were stored using custom Google Earth Engine scripts along with spatial metadata. ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nPoints were stored using custom Google Earth Engine scripts along with spatial metadata.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nPoints were stored using custom Google Earth Engine scripts along with spatial metadata.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\n-Sundarbans\n-Gulf of Khambhat\n-Gulf of Mannar\n-Pichavaram\n-Andaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nPoints were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans\nGulf of Khambhat\nGulf of Mannar\nPichavaram\nAndaman \u0026amp; Nicobar Islands\nThese areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nPoints were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included: â–¡ Open water â–¡ Urban areas â–¡ Forest â–¡ Wetlands â–¡ Barren land\nPoints were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. â–  Train-test split: â–¡ 70% training â–¡ 30% testing â–  Advantages of RF: â–¡ Resistant to overfitting â–¡ Handles non-linear relationships â–¡ Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. â–  Train-test split: â–¡ 70% training â–¡ 30% testing â–  Advantages of RF: â–¡ Resistant to overfitting â–¡ Handles non-linear relationships â–¡ Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. â–  Train-test split: â–¡ 70% training â–¡ 30% testing â–  Advantages of RF: â–¡ Resistant to overfitting â–¡ Handles non-linear relationships â–¡ Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. â–  Train-test split: â–¡ 70% training â–¡ 30% testing â–  Advantages of RF: â–¡ Resistant to overfitting â–¡ Handles non-linear relationships â–¡ Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. ","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n(a) Global Mangrove Watch Reference (b) Manually Labeled GeoPoints (c) Final Classified Overlay Figure 4.1.2: Comparison of classification process: (a) Reference from Global Mangrove Watch, (b) manually annotated geo-points used for training the classifier, and (c) final classification result overlaid as tile using the trained model.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n(a) Global Mangrove Watch Reference (b) Manually Labeled GeoPoints (c) Final Classified Overlay Figure 4.1.2: Comparison of classification process: (a) Reference from Global Mangrove Watch, (b) manually annotated geo-points used for training the classifier, and (c) final classification result overlaid as tile using the trained model.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n(a) Global Mangrove Watch Reference (b) Manually Labeled GeoPoints (c) Final Classified Overlay Figure 4.1.2: Comparison of classification process: (a) Reference from Global Mangrove Watch, (b) manually annotated geo-points used for training the classifier, and (c) final classification result overlaid as tile using the trained model.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n(a) Global Mangrove Watch Reference (b) Manually Labeled GeoPoints (c) Final Classified Overlay Figure 4.1.2: Comparison of classification process: (a) Reference from Global Mangrove Watch, (b) manually annotated geo-points used for training the classifier, and (c) final classification result overlaid as tile using the trained model.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n(a) Global Mangrove Watch Reference (b) Manually Labeled GeoPoints (c) Final Classified Overlay Figure 4.1.2: Comparison of classification process: (a) Reference from Global Mangrove Watch, (b) manually annotated geo-points used for training the classifier, and (c) final classification result overlaid as tile using the trained model.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_data_sampling.png\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n(a) Global Mangrove Watch Reference (b) Manually Labeled GeoPoints (c) Final Classified Overlay Figure 4.1.2: Comparison of classification process: (a) Reference from Global Mangrove Watch, (b) manually annotated geo-points used for training the classifier, and (c) final classification result overlaid as tile using the trained model.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"},{"content":"LinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\nThe catch: No login required, and it has to bypass anti-bot measures.\nThe Approach: Google Search Strategy But for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\nInstead of hitting LinkedIn directly, I go through Google. I used python libraries selenium and BeautifulSoup4 to build the scraper.\ndef google_worker(driver, keyword, entity, location, start_page, end_page, out_urls, lock): query = f\u0026#34;site:linkedin.com/ {keyword}\u0026#34; if location: query += f\u0026#34; {location}\u0026#34; for page in range(start_page, end_page+1): url = \u0026#34;https://www.google.com/search?q=\u0026#34; + urllib.parse.quote_plus(query) + f\u0026#34;\u0026amp;start={(page-1)*10}\u0026#34; driver.get(url); human_pause() if \u0026#34;recaptcha\u0026#34; in driver.page_source.lower(): solve_recaptcha(driver) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for a in soup.select(\u0026#34;a\u0026#34;): href = a.get(\u0026#34;href\u0026#34;) if not href or not href.startswith(\u0026#34;http\u0026#34;): continue clean = href.split(\u0026#34;\u0026amp;\u0026#34;)[0] if entity == \u0026#34;company\u0026#34; and \u0026#34;linkedin.com/company\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;posts\u0026#34; and \u0026#34;/posts/\u0026#34; in href: with lock: out_urls.append(clean) elif entity == \u0026#34;jobs\u0026#34; and \u0026#34;/jobs/\u0026#34; in href: with lock: out_urls.append(clean) The reCAPTCHA Breakthrough This indirect approach avoids LinkedIn\u0026rsquo;s anti-bot detection because Google is doing the \u0026ldquo;scraping.\u0026rdquo; But there is just one big problem, How to avoid Google\u0026rsquo;s anti-bot detection, with only two attempt on running the script google started with recaptcha! Now a bot can\u0026rsquo;t solve the recaptcha byitself! Most of Web Scrappers back off after the recaptch comes. But what if I tell you I have found the way to bypass recaptcha\ndef solve_recaptcha(driver): frames = driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;) for frame in frames: if \u0026#34;recaptcha\u0026#34; in (frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34;): driver.switch_to.frame(frame) break checkbox = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-anchor\u0026#34;)) ) checkbox.click(); human_pause(2, 4) driver.switch_to.default_content() challenge_iframe = None for frame in driver.find_elements(By.TAG_NAME, \u0026#34;iframe\u0026#34;): src = frame.get_attribute(\u0026#34;src\u0026#34;) or \u0026#34;\u0026#34; title = frame.get_attribute(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; if \u0026#34;recaptcha\u0026#34; in src and \u0026#34;anchor\u0026#34; not in src: challenge_iframe = frame; break if \u0026#34;recaptcha challenge\u0026#34; in title.lower(): challenge_iframe = frame; break if not challenge_iframe: print(\u0026#34;reCAPTCHA solved by checkbox only\u0026#34;) return True print(\u0026#34;Challenge detected â†’ switching to audio\u0026#34;) driver.switch_to.frame(challenge_iframe) audio_btn = WebDriverWait(driver, 5).until( EC.element_to_be_clickable((By.ID, \u0026#34;recaptcha-audio-button\u0026#34;)) ) audio_btn.click(); human_pause(2, 4) audio_src = WebDriverWait(driver, 5).until( EC.presence_of_element_located((By.ID, \u0026#34;audio-source\u0026#34;)) ).get_attribute(\u0026#34;src\u0026#34;) audio_file, wav_file = \u0026#34;captcha.mp3\u0026#34;, \u0026#34;captcha.wav\u0026#34; with open(audio_file, \u0026#34;wb\u0026#34;) as f: f.write(requests.get(audio_src).content) sound = AudioSegment.from_mp3(audio_file) sound.export(wav_file, format=\u0026#34;wav\u0026#34;) recognizer = sr.Recognizer() with sr.AudioFile(wav_file) as source: audio_data = recognizer.record(source) text = recognizer.recognize_google(audio_data) print(\u0026#34;ðŸŽ¤ Recognized:\u0026#34;, text) input_box = driver.find_element(By.ID, \u0026#34;audio-response\u0026#34;) input_box.send_keys(text) driver.find_element(By.ID, \u0026#34;recaptcha-verify-button\u0026#34;).click() human_pause(3, 5) driver.switch_to.default_content() print(\u0026#34; reCAPTCHA solved with audio\u0026#34;) return True So how does bot bypasses recaptcha because the pattern and images always random in recaptcha? We don\u0026rsquo;t solve recaptcha, we chose the option with icon of headphones bottom of the captcha, which is actually an accessibility for people where they can hear what the speech is and then write it in the text box. We download that speech file, use Google\u0026rsquo;s speech-to-text to convert that speech to text and then make the bot input it.\nHere\u0026rsquo;s a demo of the LinkedIn scraper in action:\nYour browser does not support the video tag. Yes its exactly like I used google to defeat google\nFeatures Not just that it can scrap posts, I add the functionality where we can scrape a company profile and jobs too. Where companies profile do require linkedin\u0026rsquo;s login otherwise we just hit linkedin authwall!\nIn companies profile, with the help of beautifulsoup4 scraper can collect images in the posts made by company, I have used human-like behaviour to avoid linkedin security measures for bot detection.\ndef scrape_company(driver, urls, output_file=\u0026#34;companies.csv\u0026#34;, img_folder=\u0026#34;images\u0026#34;): all_data = [] for url in urls[:5]: driver.get(url); human_pause(0.5, 0.8) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) # download company media images for img in soup.select(\u0026#34;img\u0026#34;): src = img.get(\u0026#34;src\u0026#34;) if src and \u0026#34;media.licdn.com\u0026#34; in src: download_image(src, img_folder, driver) name = soup.select_one(\u0026#34;h1.top-card-layout__title\u0026#34;) headline = soup.select_one(\u0026#34;h2.top-card-layout__headline\u0026#34;) location = soup.select_one(\u0026#34;h3.top-card-layout__first-subline\u0026#34;) about = soup.select_one(\u0026#34;section[data-test-id=\u0026#39;about-us\u0026#39;] p\u0026#34;) size = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__size\u0026#39;] dd\u0026#34;) industry = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__industry\u0026#39;] dd\u0026#34;) founded = soup.select_one(\u0026#34;div[data-test-id=\u0026#39;about-us__foundedOn\u0026#39;] dd\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;name\u0026#34;: name.get_text(strip=True) if name else \u0026#34;\u0026#34;, \u0026#34;headline\u0026#34;: headline.get_text(strip=True) if headline else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;about\u0026#34;: about.get_text(strip=True) if about else \u0026#34;\u0026#34;, \u0026#34;size\u0026#34;: size.get_text(strip=True) if size else \u0026#34;\u0026#34;, \u0026#34;industry\u0026#34;: industry.get_text(strip=True) if industry else \u0026#34;\u0026#34;, \u0026#34;founded\u0026#34;: founded.get_text(strip=True) if founded else \u0026#34;\u0026#34;, }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;headline\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;about\u0026#34;,\u0026#34;size\u0026#34;,\u0026#34;industry\u0026#34;,\u0026#34;founded\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} companies â†’ {output_file}\u0026#34;) def scrape_jobs(driver, urls, output_file=\u0026#34;jobs.csv\u0026#34;): all_data = [] for url in urls: driver.get(url); human_pause(0.5, 1) soup = BeautifulSoup(driver.page_source, \u0026#34;html.parser\u0026#34;) for li in soup.select(\u0026#34;ul.jobs-search__results-list li\u0026#34;): title = li.select_one(\u0026#34;h3\u0026#34;) company = li.select_one(\u0026#34;h4 a\u0026#34;) location = li.select_one(\u0026#34;.job-search-card__location\u0026#34;) date = li.select_one(\u0026#34;time\u0026#34;) all_data.append({ \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title.get_text(strip=True) if title else \u0026#34;\u0026#34;, \u0026#34;company\u0026#34;: company.get_text(strip=True) if company else \u0026#34;\u0026#34;, \u0026#34;location\u0026#34;: location.get_text(strip=True) if location else \u0026#34;\u0026#34;, \u0026#34;date\u0026#34;: date.get_text(strip=True) if date else \u0026#34;\u0026#34; }) with open(output_file, \u0026#34;w\u0026#34;, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=[\u0026#34;url\u0026#34;,\u0026#34;title\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;location\u0026#34;,\u0026#34;date\u0026#34;]) writer.writeheader(); writer.writerows(all_data) print(f\u0026#34; Saved {len(all_data)} jobs â†’ {output_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#34;LinkedIn Scraper with multiwindow + captcha solver\u0026#34;) parser.add_argument(\u0026#34;mode\u0026#34;, choices=[\u0026#34;posts\u0026#34;,\u0026#34;company\u0026#34;,\u0026#34;jobs\u0026#34;]) parser.add_argument(\u0026#34;keyword\u0026#34;, type=str) parser.add_argument(\u0026#34;--location\u0026#34;, type=str, default=None) parser.add_argument(\u0026#34;--pages\u0026#34;, type=int, default=1) parser.add_argument(\u0026#34;--output\u0026#34;, type=str, default=\u0026#34;out.csv\u0026#34;) parser.add_argument(\u0026#34;--img_folder\u0026#34;, type=str, default=\u0026#34;images\u0026#34;, help=\u0026#34;Folder to save downloaded images\u0026#34;) args = parser.parse_args() 1. Topic-Based Posts scrape first 50 pages with links of linkedin posts that google search query shows\npython linkedin_scraper.py --mode posts llms --pages 50 --output llms.csv 2. User Profiles Extract profile info without login\npython linkedin_scraper.py --mode company openai --pages 1 --output openai.csv 3. Job Listings Find Data Scientist jobs in Mumbai\npython linkedin_scraper.py --mode jobs Data Scientist mumbai --pages 1 --output DS_jobs.csv Key Takeaway Sometimes the best way to scrape a protected site is to not scrape it directly. Go through Google, use CAPTCHA-solving services, and mimic human behavior.\nThis scraper enables data extraction from LinkedIn without authentication or account requirements.\n","permalink":"http://localhost:1313/projects/linkedin-scraper/","summary":"\u003cp\u003eLinkedIn doesn\u0026rsquo;t want you scraping their data. But I needed to, not just because I was asked to but I like the challenge! What I wanted to scrape? Posts about specific topics e.g., \u0026ldquo;LLMs\u0026rdquo;,\u0026ldquo;transformers\u0026rdquo;,\u0026hellip;., anything people posts about on linkedin.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe catch\u003c/strong\u003e: No login required, and it has to bypass anti-bot measures.\u003c/p\u003e\n\u003ch2 id=\"the-approach-google-search-strategy\"\u003eThe Approach: Google Search Strategy\u003c/h2\u003e\n\u003cp\u003eBut for that purpose I need a search engine first, so I used google\u0026rsquo;s search with \u0026ldquo;site:linkedin.com/posts llms\u0026rdquo; and it gave me links to all the popular posts about llms on linkedin.\u003c/p\u003e","title":"I bypassed Google reCAPTCHA and LinkedIn's Anti-Scraping"},{"content":"Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is hard. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\nWhen I was working on the Dip social media app\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking 650ms per image on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\nWhen you\u0026rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model\u0026rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one nudenet detector and other was nsfw classifier. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.\nNudenet detector output Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose! Here:\nON GPU, for running the model, TensorRT is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you\u0026rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.\nOn CPU, for running model, consider ONNX-runtime or if using intel CPU use OpenVINO. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel\u0026rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use tflite!!!\nThe Optimization Strategy Anytime I have to reduce latency of any, I used python\u0026rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector\u0026rsquo;s input size was 640x640, and classifier\u0026rsquo;s input size was 224x224 so I needed to do resizing two times using opencv-python\nimport cv2 import imutils image = cv2.imread(\u0026#39;image.png\u0026#39;) cv2.imshow(\u0026#39;Original Image\u0026#39;, image) cv2.waitKey(0) One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.\nAs i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to 650ms to 580ms!\n1. PyTorch â†’ ONNX Conversion First step was converting from PyTorch to ONNX format. ONNX is great because it\u0026rsquo;s framework-agnostic and supports hardware acceleration.\ntorch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=11 ) Result: 580ms to 350ms, but still not enought fast.\nAs you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?\n2. ONNX â†’ OpenVINO with Model Optimizer Intel\u0026rsquo;s OpenVINO (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO\u0026rsquo;s intermediate representation.\nmo --input_model model.onnx \\ --output_dir openvino_model/ \\ --data_type FP16 Result: Another speedup. We got from 350 to 250ms!\n3. Post-Training Optimization Toolkit (POT) This is where the magic happened. OpenVINO\u0026rsquo;s POT applies advanced optimizations:\nQuantization: Converting FP32 to INT8 Graph optimization: Removing unnecessary operations Layer fusion: Combining compatible layers Why Quantization Works Quantization reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.\n# FP32: 4 bytes per parameter # FP16: 2 bytes per parameter # INT8: 1 byte per parameter from openvino.tools.pot import optimize_model optimized_model = optimize_model( model=openvino_model, engine_config=engine_config, metric=metric ) Result: Finally we got the latency to 180ms!!\nThe Numbers Metric Before After Improvement Inference Time 650ms 180ms 3.7x faster Accuracy 97% 97% Maintained Model Size 100% 60% 40% smaller The Sweet Victory Here\u0026rsquo;s the kicker: we beat Ultralytics\u0026rsquo; own benchmark. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to 180ms.\n\u0026ldquo;180ms vs Ultralytics\u0026rsquo; 200ms = 10% faster\u0026rdquo;\nThe Technical Details The accuracy impact was not even \u0026lt;0.1%.\nGraph Optimizations Applied Constant Folding: Pre-computed values at compile time Dead Code Elimination: Removed unused operations Batch Norm Fusion: Combined batch normalization with convolution layers ReLU Fusion: Merged activation functions with parent layers Different hardware requires different optimization strategies:\nIntel CPUs â†’ OpenVINO NVIDIA GPUs â†’ TensorRT Apple Silicon â†’ Core ML ARM devices â†’ TFLite The 80/20 Rule Most of the optimization gains came from two things:\nModel conversion (PyTorch â†’ ONNX â†’ OpenVINO) Quantization (FP32 â†’ FP16) These two steps gave us 85% of our performance improvement.\nTrade-offs are Manageable 3.7x faster with \u0026lt;0.1% accuracy loss is a no-brainer in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It\u0026rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.\nThis optimization was part of building Dip\u0026rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.\n","permalink":"http://localhost:1313/post/nsmfw-moderation-optimization/","summary":"\u003cp\u003eEvery Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is \u003cstrong\u003ehard\u003c/strong\u003e. You need to process hundreds of images per second, maintain high accuracy, and do it all in \u0026lt;200ms per image. That\u0026rsquo;s the sweet spot for \u0026ldquo;real-time\u0026rdquo; content filtering.\u003c/p\u003e\n\u003cp\u003eWhen I was working on the \u003ca href=\"https://dip.chat\"\u003eDip social media app\u003c/a\u003e\u0026rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking \u003cstrong\u003e650ms per image\u003c/strong\u003e on CPU. That\u0026rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.\u003c/p\u003e","title":"I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics' Own Benchmark"},{"content":"Students need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\nI scraped Google Maps and collected 3GB of business data around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used dash by Plotly to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\nAlso whatever you conclude to showcase on your dashboard should be informative and visually consistent. I decided to go with a scattermap for my first graph which shows pincodes of one selected city as interactive bubbles.\nSecond was an interactive histogram of top 50 pincodes with highest business counts. We can select a pincode from histogram or from scattermap Upon selection you can access the visualisation a business table, which can be filtered by any category you want and it will show you the top business in that category in the selected pincode based on the google maps review!! I added another feature where upon selecting a category and pincode you can download the csv file of with original filtered the exact way. First Version: Painfully Slow df = pd.read_csv(\u0026#39;businesses.csv\u0026#39;) filtered_df = df[df[\u0026#39;city\u0026#39;] == selected_city] map_plot = px.scatter_mapbox(filtered_df, ...) Result: 90+ second load time. No one would use this.\nThe Optimization Journey 1. CSV â†’ Parquet Conversion Parquet can refer to a columnar data file format optimized for big data processing or a type of decorative wooden flooring with geometric patterns. The data file format, Apache Parquet, is widely used for its efficiency in storage and querying, with benefits like efficient compression and schema evolution.\ndf.to_parquet(\u0026#39;businesses.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;) df = pd.read_parquet(\u0026#39;businesses.parquet\u0026#39;, columns=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;pincode\u0026#39;, \u0026#39;rating\u0026#39;]) Improvement: 33% faster (90s â†’ 60s)\n2. Data Type Optimization Quantization generally refers to reducing the precision of data, typically to 8-bit, 4-bit, or even lower integer or floating-point formats, to save memory and speed up computation\ndf[\u0026#39;category\u0026#39;] = df[\u0026#39;category\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;city\u0026#39;] = df[\u0026#39;city\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;pincode\u0026#39;] = pd.to_numeric(df[\u0026#39;pincode\u0026#39;], downcast=\u0026#39;integer\u0026#39;) df[\u0026#39;rating\u0026#39;] = pd.to_numeric(df[\u0026#39;rating\u0026#39;], downcast=\u0026#39;float\u0026#39;) # Memory usage: 3GB â†’ 1.2GB # Load time: 60s â†’ 30s Improvement: 2x faster (60s â†’ 30s)\n3. Flask Caching from functools import lru_cache from flask import Flask app = Flask(__name__) @lru_cache(maxsize=128) def load_data(city): return pd.read_parquet(f\u0026#39;data/{city}.parquet\u0026#39;) @app.route(\u0026#39;/get_data\u0026#39;) def get_data(): city = request.args.get(\u0026#39;city\u0026#39;) data = load_data(city) return data.to_json() Improvement: 30s â†’ 10s (for cached data)\n4. Pre-filtering at Read Time df = pd.read_parquet( \u0026#39;businesses.parquet\u0026#39;, filters=[(\u0026#39;city\u0026#39;, \u0026#39;==\u0026#39;, selected_city)] ) Result: 1 second load time!\nThe Results Metric Before After Improvement Load time 90+ seconds 1 second 90x faster Memory usage 3GB 1.2GB 60% reduction User satisfaction 0% (too slow) 95% Much better! Key Takeaways Format matters: Parquet \u0026raquo; CSV for analytics Load only what you need: Filters at read time beat filters in memory Caching is magic: LRU cache saved the day Data types: Optimization saves memory and speed The 90x speedup turned an unusable dashboard into a fast, responsive tool that students actually use.\nThis dashboard helps students find businesses and services near their colleges across 7 major Indian cities.\n","permalink":"http://localhost:1313/projects/college-hotspots-dashboard/","summary":"\u003cp\u003eStudents need to find, PGs and hostels, Restaurants and cafes, Study spaces, competitive exam classes and other student services near colleges\u003c/p\u003e\n\u003cp\u003eI scraped Google Maps and collected \u003cstrong\u003e3GB of business data\u003c/strong\u003e around colleges in 8 major Indian cities. It contained business of more than 100 categories, around all the colleges in the city. I used \u003ca href=\"%22https://plotly.com/dash/\"\u003edash by Plotly\u003c/a\u003e to start building the dashboard. To build an interactive dashboard we need to first decide what can we showcase from this data. In my dataset I had details that google maps show of any business like name, address, pincode, city, reviews, phone no., etc.\u003c/p\u003e","title":"College Hotspots Dashboard"},{"content":"Built a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with 98.5% accuracy.\nOptimized training on Param Shavak supercomputer using custom CLI scripts for data processing.\n","permalink":"http://localhost:1313/projects/individual-identification-of-mugger-crocodile/","summary":"\u003cp\u003eBuilt a YOLOv8-based model to identify 160+ individual free-ranging crocodiles from UAV imagery with \u003cstrong\u003e98.5% accuracy\u003c/strong\u003e.\u003cbr\u003e\nOptimized training on \u003cstrong\u003eParam Shavak supercomputer\u003c/strong\u003e using custom CLI scripts for data processing.\u003c/p\u003e","title":"Individual Identification of Mugger Crocodiles using YOLOv8"},{"content":"The Challenge BookMyShow is notoriously difficult to scrape. They have:\nAdvanced bot detection Dynamic JavaScript content Rate limiting CAPTCHA challenges I needed to scrape all 1,898 cities in India across 4 categories (plays, sports, events, activities).\nThe Problem First attempt: Single-threaded Python script\nResult: 3 hours per city Total time: ~6,000 hours (way too slow) # This was painfully slow for city in cities: events = scrape_city(city) # 3 hours per city! Tried multithreading, but threads running in the background got blocked by BookMyShow\u0026rsquo;s detection.\nThe Innovation: Windows Multi-Window Layout The breakthrough: instead of running threads in the background, I ran 4 windows simultaneously in the foreground.\n# Instead of this (blocked): threads = [] for url in urls: t = Thread(target=scrape, args=(url,)) t.start() # Background = blocked threads.append(t) t.join() # Do this (60x faster): for i in range(4): driver = webdriver.Chrome() driver.set_window_position(i * 400, 0) # Tile windows driver.get(urls[i]) scrape_parallel(driver, urls[i:i+4]) # 4 cities at once! Each window scraped a different set of cities simultaneously. No background thread blocking!\nThe Results Metric Before After Improvement Time per city 3 hours 3 minutes 60x faster All 1,898 cities 6,000 hours 100 hours Feasible! Anti-Detection Strategy def human_like_behavior(driver): # Human-like scrolling for _ in range(5): driver.execute_script(\u0026#34;window.scrollBy(0, 500);\u0026#34;) time.sleep(random.uniform(1, 3)) # Random delays time.sleep(random.uniform(2, 5)) # Mouse movements action = ActionChains(driver) action.move_by_offset(random.randint(0, 100), random.randint(0, 100)) action.perform() Coverage 1,898 cities across India 4 categories: Plays, Sports, Events, Activities Success rate: 94% Key Takeaway Sometimes the best optimization isn\u0026rsquo;t faster codeâ€”it\u0026rsquo;s a different approach altogether.\nInstead of fighting the system, I worked with it. Windows GUI automation beat multithreading because it looked like a real user.\nThis scraper collected comprehensive event data from India\u0026rsquo;s largest entertainment platform.\n","permalink":"http://localhost:1313/projects/bookmyshow-scraper/","summary":"\u003ch2 id=\"the-challenge\"\u003eThe Challenge\u003c/h2\u003e\n\u003cp\u003eBookMyShow is \u003cstrong\u003enotoriously difficult\u003c/strong\u003e to scrape. They have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced bot detection\u003c/li\u003e\n\u003cli\u003eDynamic JavaScript content\u003c/li\u003e\n\u003cli\u003eRate limiting\u003c/li\u003e\n\u003cli\u003eCAPTCHA challenges\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI needed to scrape \u003cstrong\u003eall 1,898 cities\u003c/strong\u003e in India across 4 categories (plays, sports, events, activities).\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eFirst attempt: Single-threaded Python script\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: 3 hours per city\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTotal time\u003c/strong\u003e: ~6,000 hours (way too slow)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# This was painfully slow\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e city \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cities:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    events \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e scrape_city(city)  \u003cspan style=\"color:#75715e\"\u003e# 3 hours per city!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTried multithreading, but threads running in the background got \u003cstrong\u003eblocked by BookMyShow\u0026rsquo;s detection\u003c/strong\u003e.\u003c/p\u003e","title":"BookMyShow Scraper - I Got 60x Speedup by Running 4 Windows Instead of Threads"},{"content":"Overview Final semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\nMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\nPersistent cloud cover Limited accessibility to remote areas Inconsistent data quality The core objective is to develop a geospatial classification model capable of differentiating mangrove and non-mangrove classes using high-resolution satellite imagery. The focus is limited to the Indian subcontinent, where coastal zones are vulnerable to ecological degradation. The classification model is designed to be scalable, cloud-based, and capable of handling spatially distributed data.\nTo manually label and validate training samples using Global Mangrove Watch data Over 570 points were manually marked for each class (mangrove = 1, non-mangrove = 0). The labeling process was guided by GMW overlays and satellite base maps. Non-mangrove classes included forests, rivers, wastelands, mudflats, and urban backgrounds to ensure the classifier learned meaningful feature distinctions.\nTo compute vegetation indices and spectral bands for model input features The following indices and bands were used to improve model learning: -NDVI = (B8 - B4) / (B8 + B4) â€“ to highlight vegetation density -NDWI = (B3 - B8) / (B3 + B8) â€“ to separate water features -MNDWI = (B3 - B11) / (B3 + B11) â€“ to enhance mudflat and wetland detection -Raw bands: B2 (Blue), B3 (Green), B4 (Red), B8 (NIR), B11 (SWIR)\nTo export the classification result as tile overlays for deployment Since GEE restricts model API deployment, the final classification image was exported as a map tile overlay using ee.Image.getMapId(), which allows integration with custom web applications.\nTo build an interactive click-to-classify frontend using Streamlit and Folium A responsive web application was created using Streamlit with embedded Folium maps. Key functionalities include: -Interactive satellite and terrain map views -Real-time click-to-classify feature using EE classification values -Visualization of mangrove (green) and non-mangrove (red) areas -Area estimation using pixel-wise aggregation of classified data\nStudy Area and Dataset Selection\nThe classification was geographically focused on the coastal regions of India, encompassing known mangrove ecosystems such as:\nSundarbans Gulf of Khambhat Gulf of Mannar Pichavaram Andaman \u0026amp; Nicobar Islands These areas were selected based on their ecological significance and confirmed mangrove presence using Global Mangrove Watch (GMW) overlays. The primary dataset used was Sentinel-2A Level-2A Surface Reflectance imagery.\nFor Data Sampling, a total of Ëœ570 points per class were manually marked.\nMangrove (class = 1): Points placed over dense mangrove patches, verified with GMW overlays and true-color images.\nNon-mangrove (class = 0): Points included:\nOpen water Urban areas Forest Wetlands Barren land Points were stored using custom Google Earth Engine scripts along with spatial metadata.\nEach point was enriched with the following features: Spectral Bands\nB2 (Blue, 10m) B3 (Green, 10m) B4 (Red, 10m) B8 (Near Infrared, 10m) B11 (Short Wave Infrared, 20m, resampled) Vegetation Indices\nNDVI = B8âˆ’B4/B8+B4 (indicates chlorophyll and biomass) NDWI = B3âˆ’B8/B3+B8 (highlights water bodies) This resulted in a 7-dimensional feature vector for each point: [B2, B3, B4, B8, B11, NDVI, NDWI].\nModel Training\nA Random Forest classifier was trained using the smileRandomForest module in GEE.\nFigure 3.4.1: Screenshot of the GEE script used for training the Random Forest classifier and exporting results. The console output on the right summarizes training-validation split, dataset balance, and feature selection used for classification. Train-test split: 70% training 30% testing Advantages of RF: Resistant to overfitting Handles non-linear relationships Offers feature importance analysis The model achieved an accuracy of 99.03%, validated using test points near the Mumbai coastline. Model Deployment\nDeployed using Streamlit, Google Earth Engine (GEE), and Folium for interactive mapping Initialized using GEE Python API, Tile layered generated with getMapId( ).\nI added Click \u0026amp; Classify functionality allows user to click a point on map and it captures Latitude and Longitude, how does it work? so Lats \u0026amp; Longs are converted to GEE point and Classification value (0 or 1) gets fetched using reduceRegion( ).\nOverall Architecture\nOutcome\nThe strong outcome validates the effectiveness of the supervised learning approach, the geo-point sampling strategy, and the value of vegetation indices like NDVI and NDWI in distinguishing mangroves from surrounding land covers.\n(a) Global Mangrove Watch Reference (b) Manually Labeled GeoPoints (c) Final Classified Overlay Figure 4.1.2: Comparison of classification process: (a) Reference from Global Mangrove Watch, (b) manually annotated geo-points used for training the classifier, and (c) final classification result overlaid as tile using the trained model.","permalink":"http://localhost:1313/projects/remote-sensing-mangrove-classification/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cimg src=\"/images/mangrove_plantation.jpg\"\u003e\n\u003cp\u003eFinal semester project focused on developing a high-accuracy land cover classification system for mangrove ecosystems using satellite imagery and cloud-based geospatial platforms. The motivation behind this project was a research about mangroves that I was part of, where I learned to fly a drone over mangrove plantations for their monitoring and plantation.\u003c/p\u003e\n\u003cp\u003eMangroves serve as vital coastal ecosystems, but their spatial distribution is often poorly mapped due to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePersistent cloud cover\u003c/li\u003e\n\u003cli\u003eLimited accessibility to remote areas\u003c/li\u003e\n\u003cli\u003eInconsistent data quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe core objective is to develop a geospatial classification model capable of differentiating\nmangrove and non-mangrove classes using high-resolution satellite imagery. The focus\nis limited to the Indian subcontinent, where coastal zones are vulnerable to ecological\ndegradation. The classification model is designed to be scalable, cloud-based, and capable\nof handling spatially distributed data.\u003c/p\u003e","title":"Remote Sensing-Based Mangrove Classification"},{"content":"Developed an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\nTechnical Implementation Image Processing Pipeline Real-time image processing with OpenCV Perspective transformation for bird\u0026rsquo;s eye view Thresholding for lane detection Histogram-based lane tracking Hardware Integration Integrated multiple sensors: IR sensors for obstacle detection Ultrasonic sensors for distance measurement Photoresistors for ambient light detection Real-time control logic for: Steering adjustments Acceleration control Automatic braking An autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\nComplementing the camera, infrared (IR) sensors enhance the car\u0026rsquo;s perceptual capabilities, especially in low-light conditions, by detecting obstacles through emitted and reflected infrared light. Ultrasonic sensors contribute to proximity sensing, utilizing sound waves to measure distances to nearby objects and aiding the car in navigation through confined spaces. Photodiode sensors play a pivotal role in maintaining the car\u0026rsquo;s trajectory by detecting variations in light intensity and ensuring alignment with lane markings.\nThe image processing model, running on a dedicated processing unit, synthesizes data from these sensors to generate a dynamic understanding of the environment. A sophisticated control system integrates this information, translating it into precise steering, acceleration, and braking commands. This amalgamation of image processing and sensor data enables the autonomous car to navigate predefined paths, respond to traffic signs, and adapt to its surroundings, marking a significant advancement in the realization of intelligent and safe autonomous transportation.\nHow the Image Processing was implemented for lane detection:\nCapturing Frames from the Camera\nThe camera is initialized with specific settings â€” frame width, height, brightness, etc. Each frame is captured and converted from BGR to RGB (since OpenCV uses BGR by default).\nvoid Capture() { Camera.grab(); Camera.retrieve(frame); cvtColor(frame, frame, COLOR_BGR2RGB); } Perspective Transformation\nWe convert the front-facing view into a birdâ€™s-eye (top-down) view using four defined source and destination points.\nThis simplifies the lane detection process since lane lines appear straight in this view.\nMatrix = getPerspectiveTransform(Source, Destination); warpPerspective(frame, framePers, Matrix, Size(400,240)); Lane Line Isolation (Thresholding + Edges)\nThe system applies grayscale conversion, thresholding, and Canny edge detection to isolate lane markings.\ncvtColor(framePers, frameGray, COLOR_RGB2GRAY); inRange(frameGray, 200, 255, frameThresh); Canny(frameGray, frameEdge, 900, 900, 3, false); add(frameThresh, frameEdge, frameFinal); Histogram Analysis\nTo find lane positions, the code scans the bottom portion of the image column by column and computes a histogram of white pixel intensities.\nfor(int i=0; i\u0026lt;400; i++) { ROILane = frameFinalDuplicate(Rect(i,140,1,100)); divide(255, ROILane, ROILane); histrogramLane.push_back((int)(sum(ROILane)[0])); } Detecting Lanes and Center Offset\nThe program identifies the maximum peaks on both sides of the histogram â€” these correspond to lane positions.\nLeftLanePos = distance(histrogramLane.begin(), max_element(histrogramLane.begin(), histrogramLane.begin() + 150)); RightLanePos = distance(histrogramLane.begin() +250, max_element(histrogramLane.begin() +250, histrogramLane.end())); Then it calculates the lane center and compares it to the frame center (188 pixels):Then it calculates the lane center and compares it to the frame center (188 pixels):\nlaneCenter = (RightLanePos - LeftLanePos)/2 + LeftLanePos; Result = laneCenter - frameCenter; Steering Decision via GPIO\nBased on the Result value, specific GPIO pins are activated to signal movement direction. Each pattern corresponds to a particular motion.\nif (Result == 0) { cout \u0026lt;\u0026lt; \u0026#34;Forward\u0026#34;; } else if (Result \u0026gt;0 \u0026amp;\u0026amp; Result \u0026lt;10) { cout \u0026lt;\u0026lt; \u0026#34;Right1\u0026#34;; } else if (Result \u0026gt;=10 \u0026amp;\u0026amp; Result \u0026lt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right2\u0026#34;; } else if (Result \u0026gt;20) { cout \u0026lt;\u0026lt; \u0026#34;Right3\u0026#34;; } else if (Result \u0026lt;0 \u0026amp;\u0026amp; Result \u0026gt;-10) { cout \u0026lt;\u0026lt; \u0026#34;Left1\u0026#34;; } else if (Result \u0026lt;=-10 \u0026amp;\u0026amp; Result \u0026gt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left2\u0026#34;; } else if (Result \u0026lt;-20) { cout \u0026lt;\u0026lt; \u0026#34;Left3\u0026#34;; } Visualization and FPS\nThe system displays three live OpenCV windows:\nOriginal View (raw camera feed)\nPerspective View (birdâ€™s-eye transformation)\nFinal View (processed lane lines)\nIt also calculates and prints real-time FPS to monitor performance.\nResults:\nDetects lanes in real-time at ~15â€“20 FPS on Raspberry Pi Outputs steering directions instantly Can be integrated with an RC car or motor driver for autonomous navigation\n","permalink":"http://localhost:1313/projects/real-time-self-driving-car/","summary":"\u003cp\u003eDeveloped an autonomous self-driving car using Raspberry Pi with advanced computer vision capabilities for lane detection and obstacle avoidance.\u003c/p\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"image-processing-pipeline\"\u003eImage Processing Pipeline\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eReal-time image processing\u003c/strong\u003e with OpenCV\u003c/li\u003e\n\u003cli\u003ePerspective transformation for bird\u0026rsquo;s eye view\u003c/li\u003e\n\u003cli\u003eThresholding for lane detection\u003c/li\u003e\n\u003cli\u003eHistogram-based lane tracking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hardware-integration\"\u003eHardware Integration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIntegrated multiple sensors:\n\u003cul\u003e\n\u003cli\u003eIR sensors for obstacle detection\u003c/li\u003e\n\u003cli\u003eUltrasonic sensors for distance measurement\u003c/li\u003e\n\u003cli\u003ePhotoresistors for ambient light detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReal-time control logic for:\n\u003cul\u003e\n\u003cli\u003eSteering adjustments\u003c/li\u003e\n\u003cli\u003eAcceleration control\u003c/li\u003e\n\u003cli\u003eAutomatic braking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn autonomous car with image processing employs a sophisticated network of sensors to navigate its surroundings intelligently. Central to this system is a camera that captures real-time images, providing the foundational visual input for the car. These images undergo intricate image processing, facilitated by advanced algorithms, to recognize crucial elements such as lane markings, traffic signs, and obstacles.\u003c/p\u003e","title":"Real-Time Self-Driving Car with Lane Detection"}]