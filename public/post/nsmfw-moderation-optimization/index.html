<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics&#39; Own Benchmark | Shubham Apat</title>
<meta name="keywords" content="deep-learning, model-optimization, openvino, performance, content-moderation">
<meta name="description" content="How I optimized YOLOv8&#39;s detection from 650ms to 180ms using OpenVINO, even beating Ultralytics&#39; own benchmarks">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/post/nsmfw-moderation-optimization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b4c2d92c2be113ac5a84dc380901003f2b5cda3766f3a1e3e17c03aa160da3b5.css" integrity="sha256-tMLZLCvhE6xahNw4CQEAPytc2jdm86Hj4XwDqhYNo7U=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/post/nsmfw-moderation-optimization/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Shubham Apat (Alt + H)">Shubham Apat</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/post/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/shubhamapat7" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      I Made YOLOv8 Detection 3.7x Faster and Beat Ultralytics&#39; Own Benchmark
    </h1>
    <div class="post-description">
      How I optimized YOLOv8&#39;s detection from 650ms to 180ms using OpenVINO, even beating Ultralytics&#39; own benchmarks
    </div>
    <div class="post-meta"><span title='2024-09-01 00:00:00 +0000 UTC'>September 1, 2024</span>&nbsp;·&nbsp;<span>5 min</span>

</div>
  </header> 
  <div class="post-content"><p>Every Social Media app needs Content moderation. At ByteCitadel I was given the task to make the Image Moderation pipeline to not let users to post NSFW content. But content moderation at scale is <strong>hard</strong>. You need to process hundreds of images per second, maintain high accuracy, and do it all in &lt;200ms per image. That&rsquo;s the sweet spot for &ldquo;real-time&rdquo; content filtering.</p>
<p>When I was working on the <a href="https://dip.chat">Dip social media app</a>&rsquo;s NSFW detection pipeline, we hit a bottleneck: the whole pipeline was taking <strong>650ms per image</strong> on CPU. That&rsquo;s too slow for real-time use, especially when users are uploading photos rapidly.</p>
<p>When you&rsquo;re working on a pipeline with Image input and its prediction, the resource heavy and time consuming constraints to look out for are image resizing, preprocessing the input before the model prediction, and model&rsquo;s inference speed!!! Now why my pipeline was taking 650ms per image on CPU? I used two models, one <a href="https://github.com/notAI-tech/NudeNet/blob/v3/README.md">nudenet detector</a> and other was <a href="https://github.com/alex000kim/nsfw_data_scraper">nsfw classifier</a>. The nsfw classifier was a mobilenetv2small.tflite and the detector was yolov8m.pt.</p>
<img src="/images/nudenet.png">
<center>Nudenet detector output</center>
<p>Things you should know for edge optimizations for your model at production levels, first what framework is your model saved in and which format is the best for your purpose!
Here:</p>
<p>ON GPU, for running the model, <a href="https://developer.nvidia.com/tensorrt#:~:text=NVIDIA%C2%AE%20TensorRT%E2%84%A2%20is,high%20throughput%20for%20production%20applications.">TensorRT</a> is currently the fastest format, it is an NVIDIA library specifically designed to maximize inference performance on NVIDIA GPUs through a series of aggressive optimizations. But if you&rsquo;re using CPU then TensorRT is not useful at all because it is a GPU specific library.</p>
<p>On CPU, for running model, consider <a href="https://onnx.ai/">ONNX-runtime</a> or if using intel CPU use <a href="https://github.com/openvinotoolkit/openvino">OpenVINO</a>. OpenVINO is typically the top performer on Intel CPUs, as it is specifically tailored to leverage Intel&rsquo;s architecture and instruction sets, whereas ONNX Runtime offers excellent, highly portable CPU performance across various hardware (including non-Intel CPUs) and is a strong general-purpose choice for cross-platform deployment. But if your main goal is to deploy the model on a mobile app, use <a href="https://ai.google.dev/edge/litert">tflite</a>!!!</p>
<img src="/images/comparison_of_models.png">
<h2 id="the-optimization-strategy">The Optimization Strategy<a hidden class="anchor" aria-hidden="true" href="#the-optimization-strategy">#</a></h2>
<p>Anytime I have to reduce latency of any, I used python&rsquo;s time library on every function of the pipeline to understand which is the most time consuming. In my pipeline, both the models had different input sizes, the detector&rsquo;s input size was 640x640, and classifier&rsquo;s input size was 224x224 so I needed to do resizing two times using <strong>opencv-python</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> cv2
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> imutils
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;image.png&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cv2<span style="color:#f92672">.</span>imshow(<span style="color:#e6db74">&#39;Original Image&#39;</span>, image)
</span></span><span style="display:flex;"><span>cv2<span style="color:#f92672">.</span>waitKey(<span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>One thing you should know about opencv is how ridiculously slow it is at resizing an image, and top of it I had to do it twice! So the zero step I took was running both the models in parallel using threading, and reduce reads and writes by making the code modular! Adding abstraction to code seriously helps you find out the unnecessary calls happening to other unwanted functions for one task.</p>
<p>As i made my code modular and ran both models in parallel, the latency of whole pipeline reduced to <strong>650ms to 580ms</strong>!</p>
<h3 id="1-pytorch--onnx-conversion">1. PyTorch → ONNX Conversion<a hidden class="anchor" aria-hidden="true" href="#1-pytorch--onnx-conversion">#</a></h3>
<p>First step was converting from PyTorch to <a href="https://onnx.ai/">ONNX</a> format. ONNX is great because it&rsquo;s framework-agnostic and supports hardware acceleration.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(
</span></span><span style="display:flex;"><span>    model,
</span></span><span style="display:flex;"><span>    dummy_input,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;model.onnx&#34;</span>,
</span></span><span style="display:flex;"><span>    export_params<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    opset_version<span style="color:#f92672">=</span><span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><strong>Result</strong>: 580ms to 350ms, but still not enought fast.</p>
<img src="/images/yolov8.png">
<p>As you can see in the above table here, the YOLOv8m model has minimum latency of 234.7ms on ONNX-Runtime. With that latency and add ons of classifier and image resizing, my overall latency was 350ms! I still needed to optimize it further to run it under 200ms but I how do I best the ultralytics own benchmarks on cpu?</p>
<h3 id="2-onnx--openvino-with-model-optimizer">2. ONNX → OpenVINO with Model Optimizer<a hidden class="anchor" aria-hidden="true" href="#2-onnx--openvino-with-model-optimizer">#</a></h3>
<p>Intel&rsquo;s <a href="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html">OpenVINO</a> (Open Visual Inference and Neural network Optimization) is specifically designed for CPU optimization. The Model Optimizer (MO) tool converts ONNX models to OpenVINO&rsquo;s intermediate representation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mo --input_model model.onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   --output_dir openvino_model/ <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>   --data_type FP16
</span></span></code></pre></div><p><strong>Result</strong>: Another speedup. We got from 350 to 250ms!</p>
<h3 id="3-post-training-optimization-toolkit-pot">3. Post-Training Optimization Toolkit (POT)<a hidden class="anchor" aria-hidden="true" href="#3-post-training-optimization-toolkit-pot">#</a></h3>
<p>This is where the magic happened. OpenVINO&rsquo;s <a href="https://docs.openvino.ai/latest/pot_introduction.html">POT</a> applies advanced optimizations:</p>
<ul>
<li><strong>Quantization</strong>: Converting FP32 to INT8</li>
<li><strong>Graph optimization</strong>: Removing unnecessary operations</li>
<li><strong>Layer fusion</strong>: Combining compatible layers</li>
</ul>
<h3 id="why-quantization-works">Why Quantization Works<a hidden class="anchor" aria-hidden="true" href="#why-quantization-works">#</a></h3>
<p><a href="https://pytorch.org/docs/stable/quantization.html">Quantization</a> reduces precision from 32-bit floats to 16-bit (or 8-bit) floats. This reduces memory bandwidth and computation time.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># FP32: 4 bytes per parameter</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># FP16: 2 bytes per parameter</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INT8: 1 byte per parameter</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> openvino.tools.pot <span style="color:#f92672">import</span> optimize_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>optimized_model <span style="color:#f92672">=</span> optimize_model(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>openvino_model,
</span></span><span style="display:flex;"><span>    engine_config<span style="color:#f92672">=</span>engine_config,
</span></span><span style="display:flex;"><span>    metric<span style="color:#f92672">=</span>metric
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><strong>Result</strong>: Finally we got the latency to <strong>180ms</strong>!!</p>
<h2 id="the-numbers">The Numbers<a hidden class="anchor" aria-hidden="true" href="#the-numbers">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Before</th>
          <th>After</th>
          <th>Improvement</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Inference Time</strong></td>
          <td>650ms</td>
          <td>180ms</td>
          <td><strong>3.7x faster</strong></td>
      </tr>
      <tr>
          <td><strong>Accuracy</strong></td>
          <td>97%</td>
          <td>97%</td>
          <td>Maintained</td>
      </tr>
      <tr>
          <td><strong>Model Size</strong></td>
          <td>100%</td>
          <td>60%</td>
          <td>40% smaller</td>
      </tr>
  </tbody>
</table>
<h2 id="the-sweet-victory">The Sweet Victory<a hidden class="anchor" aria-hidden="true" href="#the-sweet-victory">#</a></h2>
<p>Here&rsquo;s the kicker: we beat <strong>Ultralytics&rsquo; own benchmark</strong>. Their official docs claimed ~234.7ms for NudeNet inference. We got it down to <strong>180ms</strong>.</p>
<blockquote>
<p><strong>&ldquo;180ms vs Ultralytics&rsquo; 200ms = 10% faster&rdquo;</strong></p>
</blockquote>
<h2 id="the-technical-details">The Technical Details<a hidden class="anchor" aria-hidden="true" href="#the-technical-details">#</a></h2>
<p>The accuracy impact was not even &lt;0.1%.</p>
<h3 id="graph-optimizations-applied">Graph Optimizations Applied<a hidden class="anchor" aria-hidden="true" href="#graph-optimizations-applied">#</a></h3>
<ol>
<li><strong>Constant Folding</strong>: Pre-computed values at compile time</li>
<li><strong>Dead Code Elimination</strong>: Removed unused operations</li>
<li><strong>Batch Norm Fusion</strong>: Combined batch normalization with convolution layers</li>
<li><strong>ReLU Fusion</strong>: Merged activation functions with parent layers</li>
</ol>
<p>Different hardware requires different optimization strategies:</p>
<ul>
<li><strong>Intel CPUs</strong> → OpenVINO</li>
<li><strong>NVIDIA GPUs</strong> → <a href="https://developer.nvidia.com/tensorrt">TensorRT</a></li>
<li><strong>Apple Silicon</strong> → <a href="https://developer.apple.com/documentation/coreml">Core ML</a></li>
<li><strong>ARM devices</strong> → <a href="https://www.tensorflow.org/lite">TFLite</a></li>
</ul>
<h3 id="the-8020-rule"><strong>The 80/20 Rule</strong><a hidden class="anchor" aria-hidden="true" href="#the-8020-rule">#</a></h3>
<p>Most of the optimization gains came from two things:</p>
<ul>
<li>Model conversion (PyTorch → ONNX → OpenVINO)</li>
<li>Quantization (FP32 → FP16)</li>
</ul>
<p>These two steps gave us 85% of our performance improvement.</p>
<h3 id="trade-offs-are-manageable"><strong>Trade-offs are Manageable</strong><a hidden class="anchor" aria-hidden="true" href="#trade-offs-are-manageable">#</a></h3>
<p>3.7x faster with &lt;0.1% accuracy loss is a <strong>no-brainer</strong> in production. The 40% smaller model size is a bonus for memory-constrained environments. The 180ms NSFW detection pipeline now powers real-time content moderation for thousands of users daily. It&rsquo;s a reminder that with the right tools and techniques, you can achieve production-ready performance even with complex deep learning models.</p>
<hr>
<p><em>This optimization was part of building Dip&rsquo;s content moderation pipeline. If you found this useful, you can reach out for consulting work on model optimization and performance tuning.</em></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="http://localhost:1313/tags/model-optimization/">Model-Optimization</a></li>
      <li><a href="http://localhost:1313/tags/openvino/">Openvino</a></li>
      <li><a href="http://localhost:1313/tags/performance/">Performance</a></li>
      <li><a href="http://localhost:1313/tags/content-moderation/">Content-Moderation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/projects/linkedin-scraper/">
    <span class="title">« Prev</span>
    <br>
    <span>I bypassed Google reCAPTCHA and LinkedIn&#39;s Anti-Scraping</span>
  </a>
  <a class="next" href="http://localhost:1313/projects/college-hotspots-dashboard/">
    <span class="title">Next »</span>
    <br>
    <span>College Hotspots Dashboard</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Shubham Apat</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
